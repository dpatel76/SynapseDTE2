#!/usr/bin/env python3
"""
Create Alembic Migration from Generated SQL
==========================================

This script converts the SQL generated by database_migration_reconciler.py
into proper Alembic migration files.
"""

import os
import sys
import re
from datetime import datetime
from pathlib import Path
import argparse
import json

# Template for Alembic migration file
ALEMBIC_MIGRATION_TEMPLATE = '''"""
{message}

Revision ID: {revision_id}
Revises: {down_revision}
Create Date: {create_date}

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
import json

# revision identifiers, used by Alembic.
revision = '{revision_id}'
down_revision = {down_revision}
branch_labels = None
depends_on = None


def upgrade():
    """
    Apply the migration - create tables and insert seed data
    """
{upgrade_content}


def downgrade():
    """
    Revert the migration - drop tables
    """
{downgrade_content}
'''

class AlembicMigrationCreator:
    """Convert SQL scripts to Alembic migrations"""
    
    def __init__(self, sql_file_path: str, alembic_dir: str = "alembic"):
        self.sql_file_path = sql_file_path
        self.alembic_dir = alembic_dir
        self.versions_dir = os.path.join(alembic_dir, "versions")
        self.tables = []
        self.seed_data = {}
        self.indexes = []
        self.constraints = []
        
    def parse_sql_file(self):
        """Parse SQL file and extract components"""
        with open(self.sql_file_path, 'r') as f:
            content = f.read()
        
        # Extract CREATE TABLE statements
        create_table_pattern = r'CREATE TABLE (\w+) \((.*?)\);'
        create_matches = re.findall(create_table_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for table_name, table_def in create_matches:
            self.tables.append({
                'name': table_name,
                'definition': table_def.strip()
            })
        
        # Extract INSERT statements for seed data
        insert_pattern = r'INSERT INTO (\w+) \((.*?)\) VALUES \((.*?)\);'
        insert_matches = re.findall(insert_pattern, content, re.DOTALL)
        
        for table_name, columns, values in insert_matches:
            if table_name not in self.seed_data:
                self.seed_data[table_name] = []
            
            self.seed_data[table_name].append({
                'columns': [c.strip() for c in columns.split(',')],
                'values': values
            })
        
        # Extract CREATE INDEX statements
        index_pattern = r'CREATE\s+(?:UNIQUE\s+)?INDEX\s+(\w+)\s+ON\s+(\w+)\s*\((.*?)\);'
        index_matches = re.findall(index_pattern, content, re.IGNORECASE)
        
        for index_name, table_name, columns in index_matches:
            self.indexes.append({
                'name': index_name,
                'table': table_name,
                'columns': columns
            })
        
        # Extract ALTER TABLE statements for constraints
        alter_pattern = r'ALTER TABLE (\w+) ADD CONSTRAINT (\w+) (.*?);'
        alter_matches = re.findall(alter_pattern, content, re.DOTALL)
        
        for table_name, constraint_name, constraint_def in alter_matches:
            self.constraints.append({
                'table': table_name,
                'name': constraint_name,
                'definition': constraint_def
            })
    
    def convert_to_sqlalchemy(self, table_def: str, table_name: str) -> str:
        """Convert SQL table definition to SQLAlchemy format"""
        lines = []
        lines.append(f"    op.create_table('{table_name}',")
        
        # Parse column definitions
        column_pattern = r'(\w+)\s+([^,]+?)(?:,|$)'
        columns = re.findall(column_pattern, table_def)
        
        for col_name, col_def in columns:
            col_def = col_def.strip()
            
            # Skip constraint definitions
            if any(keyword in col_def.upper() for keyword in ['PRIMARY KEY', 'FOREIGN KEY', 'CONSTRAINT', 'CHECK']):
                continue
            
            # Convert SQL types to SQLAlchemy
            sa_column = self._convert_column_type(col_name, col_def)
            if sa_column:
                lines.append(f"        {sa_column},")
        
        lines.append("    )")
        return '\n'.join(lines)
    
    def _convert_column_type(self, col_name: str, col_def: str) -> str:
        """Convert SQL column definition to SQLAlchemy column"""
        # Map SQL types to SQLAlchemy types
        type_mapping = {
            'INTEGER': 'sa.Integer()',
            'BIGINT': 'sa.BigInteger()',
            'SMALLINT': 'sa.SmallInteger()',
            'VARCHAR': 'sa.String',
            'TEXT': 'sa.Text()',
            'BOOLEAN': 'sa.Boolean()',
            'DATE': 'sa.Date()',
            'TIMESTAMP': 'sa.DateTime()',
            'TIMESTAMP WITH TIME ZONE': 'sa.DateTime(timezone=True)',
            'JSONB': 'postgresql.JSONB()',
            'JSON': 'postgresql.JSON()',
            'UUID': 'postgresql.UUID()',
            'NUMERIC': 'sa.Numeric',
            'DECIMAL': 'sa.Decimal',
            'REAL': 'sa.Float()',
            'DOUBLE PRECISION': 'sa.Float()',
        }
        
        # Extract type and modifiers
        col_def_upper = col_def.upper()
        sa_type = None
        
        for sql_type, sa_type_str in type_mapping.items():
            if col_def_upper.startswith(sql_type):
                # Handle types with parameters
                if sql_type in ['VARCHAR', 'NUMERIC', 'DECIMAL']:
                    match = re.search(r'\((\d+)(?:,\s*(\d+))?\)', col_def)
                    if match:
                        if sql_type == 'VARCHAR':
                            sa_type = f"{sa_type_str}({match.group(1)})"
                        else:
                            precision = match.group(1)
                            scale = match.group(2) or '0'
                            sa_type = f"{sa_type_str}({precision}, {scale})"
                    else:
                        sa_type = f"{sa_type_str}()" if sql_type == 'VARCHAR' else sa_type_str
                else:
                    sa_type = sa_type_str
                break
        
        if not sa_type:
            # Default to String for unknown types
            sa_type = 'sa.String()'
        
        # Build column definition
        parts = [f"sa.Column('{col_name}'", sa_type]
        
        # Add modifiers
        if 'NOT NULL' in col_def_upper:
            parts.append('nullable=False')
        if 'PRIMARY KEY' in col_def_upper:
            parts.append('primary_key=True')
        if 'DEFAULT' in col_def_upper:
            default_match = re.search(r'DEFAULT\s+([^\s,]+)', col_def, re.IGNORECASE)
            if default_match:
                default_val = default_match.group(1)
                if default_val.upper() in ['TRUE', 'FALSE']:
                    parts.append(f'default={default_val.capitalize()}')
                elif default_val.upper() == 'NOW()':
                    parts.append('server_default=sa.func.now()')
                elif default_val.isdigit():
                    parts.append(f'default={default_val}')
                else:
                    parts.append(f"default={default_val}")
        
        return ', '.join(parts) + ')'
    
    def generate_seed_data_operations(self) -> list:
        """Generate operations for inserting seed data"""
        operations = []
        
        for table_name, inserts in self.seed_data.items():
            operations.append(f"\n    # Insert seed data for {table_name}")
            
            for insert in inserts:
                # Parse values - handle different data types
                parsed_values = self._parse_insert_values(insert['values'])
                
                # Create bulk insert operation
                operations.append(f"    op.bulk_insert(")
                operations.append(f"        sa.table('{table_name}',")
                
                # Add column definitions for the insert
                for col in insert['columns']:
                    operations.append(f"            sa.column('{col}'),")
                
                operations.append("        ),")
                operations.append("        [")
                
                # Format the values dict
                value_dict = {}
                for i, col in enumerate(insert['columns']):
                    if i < len(parsed_values):
                        value_dict[col] = parsed_values[i]
                
                operations.append(f"            {value_dict},")
                operations.append("        ]")
                operations.append("    )")
        
        return operations
    
    def _parse_insert_values(self, values_str: str) -> list:
        """Parse INSERT VALUES string into Python values"""
        values = []
        
        # Simple parser for common value types
        # This is a basic implementation - you might need to enhance it
        parts = []
        current_part = ''
        in_quotes = False
        quote_char = None
        
        for char in values_str:
            if char in ["'", '"'] and not in_quotes:
                in_quotes = True
                quote_char = char
                current_part += char
            elif char == quote_char and in_quotes:
                in_quotes = False
                quote_char = None
                current_part += char
            elif char == ',' and not in_quotes:
                parts.append(current_part.strip())
                current_part = ''
            else:
                current_part += char
        
        if current_part:
            parts.append(current_part.strip())
        
        # Convert string values to appropriate Python types
        for part in parts:
            part = part.strip()
            if part.startswith("'") and part.endswith("'"):
                # String value
                values.append(part[1:-1].replace("''", "'"))
            elif part.upper() == 'TRUE':
                values.append(True)
            elif part.upper() == 'FALSE':
                values.append(False)
            elif part.upper() == 'NULL':
                values.append(None)
            elif part.replace('.', '').replace('-', '').isdigit():
                # Number
                if '.' in part:
                    values.append(float(part))
                else:
                    values.append(int(part))
            else:
                # Keep as string
                values.append(part)
        
        return values
    
    def generate_migration_file(self, message: str, down_revision: str = None):
        """Generate the Alembic migration file"""
        # Create revision ID
        revision_id = datetime.now().strftime('%Y%m%d%H%M%S')
        
        # Build upgrade operations
        upgrade_ops = []
        
        # Create tables
        for table in self.tables:
            upgrade_ops.append(self.convert_to_sqlalchemy(table['definition'], table['name']))
            upgrade_ops.append('')
        
        # Add indexes
        if self.indexes:
            upgrade_ops.append("    # Create indexes")
            for index in self.indexes:
                upgrade_ops.append(
                    f"    op.create_index('{index['name']}', '{index['table']}', "
                    f"[{index['columns']}])"
                )
            upgrade_ops.append('')
        
        # Add foreign key constraints
        if self.constraints:
            upgrade_ops.append("    # Add foreign key constraints")
            for constraint in self.constraints:
                upgrade_ops.append(
                    f"    op.create_foreign_key('{constraint['name']}', "
                    f"'{constraint['table']}', {constraint['definition']})"
                )
            upgrade_ops.append('')
        
        # Add seed data
        if self.seed_data:
            upgrade_ops.extend(self.generate_seed_data_operations())
        
        # Build downgrade operations
        downgrade_ops = []
        for table in reversed(self.tables):
            downgrade_ops.append(f"    op.drop_table('{table['name']}')")
        
        # Format the migration content
        migration_content = ALEMBIC_MIGRATION_TEMPLATE.format(
            message=message,
            revision_id=revision_id,
            down_revision=f"'{down_revision}'" if down_revision else 'None',
            create_date=datetime.now().isoformat(),
            upgrade_content='\n'.join(upgrade_ops),
            downgrade_content='\n'.join(downgrade_ops)
        )
        
        # Write migration file
        migration_filename = f"{revision_id}_{message.lower().replace(' ', '_')}.py"
        migration_path = os.path.join(self.versions_dir, migration_filename)
        
        # Ensure versions directory exists
        os.makedirs(self.versions_dir, exist_ok=True)
        
        with open(migration_path, 'w') as f:
            f.write(migration_content)
        
        print(f"✓ Created Alembic migration: {migration_path}")
        return migration_path
    
    def create_data_migration(self, reconciliation_report_path: str = None):
        """Create a separate data migration based on reconciliation report"""
        if not reconciliation_report_path or not os.path.exists(reconciliation_report_path):
            print("No reconciliation report provided, skipping data migration")
            return
        
        with open(reconciliation_report_path, 'r') as f:
            report = json.load(f)
        
        if report['data_differences']:
            # Create a data-only migration for fixing differences
            revision_id = datetime.now().strftime('%Y%m%d%H%M%S') + '_data'
            
            operations = []
            operations.append("    # Fix data differences found during reconciliation")
            
            for diff in report['data_differences']:
                if diff['type'] == 'missing_records':
                    operations.append(
                        f"    # TODO: Insert missing records in {diff['table']} "
                        f"with primary keys: {diff['primary_keys']}"
                    )
                elif diff['type'] == 'data_mismatch':
                    operations.append(
                        f"    # TODO: Update {diff['table']}.{diff['column']} "
                        f"for record with PK={diff['primary_key']}"
                    )
            
            # Write data migration file
            data_migration = ALEMBIC_MIGRATION_TEMPLATE.format(
                message="Fix data discrepancies from reconciliation",
                revision_id=revision_id,
                down_revision=f"'{datetime.now().strftime('%Y%m%d%H%M%S')}'",
                create_date=datetime.now().isoformat(),
                upgrade_content='\n'.join(operations),
                downgrade_content="    # Data migrations are typically not reversible\n    pass"
            )
            
            migration_path = os.path.join(
                self.versions_dir, 
                f"{revision_id}_fix_data_discrepancies.py"
            )
            
            with open(migration_path, 'w') as f:
                f.write(data_migration)
            
            print(f"✓ Created data fix migration: {migration_path}")


def main():
    parser = argparse.ArgumentParser(
        description='Convert SQL migration to Alembic migration files'
    )
    parser.add_argument(
        'sql_file',
        help='Path to SQL migration file generated by database_migration_reconciler.py'
    )
    parser.add_argument(
        '--message', '-m',
        default='Database schema migration',
        help='Migration message/description'
    )
    parser.add_argument(
        '--alembic-dir',
        default='alembic',
        help='Path to Alembic directory (default: alembic)'
    )
    parser.add_argument(
        '--down-revision',
        help='Down revision ID (leave empty for initial migration)'
    )
    parser.add_argument(
        '--reconciliation-report',
        help='Path to reconciliation report JSON for creating data fix migration'
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(args.sql_file):
        print(f"Error: SQL file not found: {args.sql_file}")
        sys.exit(1)
    
    print(f"=== Converting SQL to Alembic Migration ===")
    print(f"SQL File: {args.sql_file}")
    print(f"Alembic Directory: {args.alembic_dir}")
    print()
    
    # Create converter
    converter = AlembicMigrationCreator(args.sql_file, args.alembic_dir)
    
    # Parse SQL file
    print("Parsing SQL file...")
    converter.parse_sql_file()
    print(f"✓ Found {len(converter.tables)} tables")
    print(f"✓ Found {len(converter.seed_data)} tables with seed data")
    print(f"✓ Found {len(converter.indexes)} indexes")
    print(f"✓ Found {len(converter.constraints)} constraints")
    print()
    
    # Generate main migration
    print("Generating Alembic migration...")
    migration_path = converter.generate_migration_file(
        args.message,
        args.down_revision
    )
    
    # Generate data fix migration if needed
    if args.reconciliation_report:
        print("\nGenerating data fix migration...")
        converter.create_data_migration(args.reconciliation_report)
    
    print("\n✓ Migration creation complete!")
    print("\nTo apply the migration:")
    print("  alembic upgrade head")
    print("\nTo review the migration:")
    print(f"  cat {migration_path}")


if __name__ == "__main__":
    main()